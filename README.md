# lookup

### still unclear / set aside for now
* token unigrams/bigrams (= [n-grams](https://en.wikipedia.org/wiki/N-gram)?)
* CQA analysis ([reference](https://research.google.com/pubs/pub37119.html) does not contain string "CQA")
* hierarchical softmax ([quora](https://www.quora.com/What-is-hierarchical-softmax))
* skip bigram model ([bigram (wiki](https://en.wikipedia.org/wiki/Bigram) "Gappy bigrams or skipping bigrams are word pairs which allow gaps (perhaps avoiding connecting words, or allowing some simulation of dependencies, as in a dependency grammar)."?)
* distributed representation ([quora](https://www.quora.com/Deep-Learning-What-is-meant-by-a-distributed-representation))
* AUR[OC]
    * [stats.stackexchange.com](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it)
    * [eli5](https://www.reddit.com/r/MachineLearning/comments/3zj4gf/eli5_receiver_operating_characteristic/)
* Kolmogorov–Smirnov test
    * [wiki](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test)
* CDF
    * = Cumulative distribution function
    * [wiki](https://en.wikipedia.org/wiki/Cumulative_distribution_function)
* [p](https://en.wikipedia.org/wiki/P-value), [Cohen's d](https://en.wikiversity.org/wiki/Cohen%27s_d)

### NLP
* distributional semantics
    * [wiki](https://en.wikipedia.org/wiki/Distributional_semantics)
        * *linguistic items with similar distributions have similar meanings*
        * i.e. *words that are used and occur in the same contexts tend to purport similar meanings*
        * i.e. *a word is characterized by the company it keeps*
    * [Word embedding (wiki)](https://en.wikipedia.org/wiki/Word_embedding)
        * "words or phrases from the vocabulary are mapped to vectors of real numbers"
        * "Methods to generate this mapping include [...] explicit representation in terms of the context in which words appear."
* word sense disambiguation
    * [wiki](https://en.wikipedia.org/wiki/Word-sense_disambiguation)
        * "identifying which sense of a word (i.e. meaning) is used in a sentence, when the word has multiple meanings"
* POS tags
    * [Part-of-speech tagging (wiki)](https://en.wikipedia.org/wiki/Part-of-speech_tagging)
        * aka *grammatical tagging*, *word-category disambiguation*
        * simplified: "identification of words as nouns, verbs, adjectives, adverbs, etc."

### ML
* support vector machine
    * [wiki](https://en.wikipedia.org/wiki/Support_vector_machine)

### Measures
* Fleiss' kappa
    * [wiki](https://en.wikipedia.org/wiki/Fleiss%27_kappa)
         * "statistical measure for assessing the reliability of agreement between a fixed number of raters when assigning categorical ratings to a number of items or classifying items"
* f-score
    * [wiki](https://en.wikipedia.org/wiki/F1_score)
        * "can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst at 0"
* cross-validation
    * [chunk data and rotate chunks for traning/validation](https://i.stack.imgur.com/1fXzJ.png)
    * [wiki](https://en.wikipedia.org/wiki/Cross-validation_(statistics))
* Levenshtein distance
    * [wiki](https://en.wikipedia.org/wiki/Levenshtein_distance)
        * "the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other"
        * = InfoRet "edit distance"?

### Network analysis
* Louvain Modularity
    * [wiki](https://en.wikipedia.org/wiki/Louvain_Modularity)
    * "method to extract communities from large networks"

### Software etc.
* Vowpal Wabbit
    * [website](http://hunch.net/~vw/)
    * [github](https://github.com/JohnLangford/vowpal_wabbit/wiki)
    * a machine learning system
